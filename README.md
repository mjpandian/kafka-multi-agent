# ðŸ§  AI-Mesh: Kafka-Driven Multi-Agent System
An event-driven, self-healing AI ecosystem where specialized agents collaborate via Apache Kafka to generate, solve, and critique coding tasks.

## ðŸš€ Key Features
- **3-Agent Chain:** Producer -> Consumer -> Reviewer.
- **Self-Healing:** Retries connections until Kafka/Ollama are ready.
- **Adaptive Throttling (NEW):** The Producer monitors Consumer Lag. If the backlog exceeds 10 tasks, the Producer automatically pauses to allow the workers to catch up.

- **Intelligent Agent Chain:** Producer -> Consumer -> Reviewer-> Persistence
 - **Adaptive Throttling (The Governor):** The Producer monitors Consumer Lag in real-time. If the backlog exceeds 10 tasks, the Producer pauses to prevent system saturation.Horizontal Scaling: Supports multiple consumer-agent instances to process tasks in parallel without naming conflicts.Live 
 - **Observability:** 3-column dashboard with real-time lag metrics and status indicators.Data Persistence: Automatically archives every successful AI cycle to a local CSV file.


## ðŸ“ˆ Monitoring the "Governor"
Open the **Dashboard** (`localhost:5000`).
1. Watch the **Lag Meter**.
2. When Lag hits **11**, you will see the **Producer Column** stop updating.
3. Once the **Consumer Column** processes the backlog and Lag drops, the Producer will resume.

## ðŸ› ï¸ Architecture: The Feedback Loop
The system uses a "Closed Loop" control pattern. By querying the `KafkaAdminClient`, the Producer stays aware of the Consumer's health, preventing the common "Message Flooding" issue in AI pipelines.

## ðŸ”„ The Agent Workflow
1.  **Producer:** Brainstorms a coding task using LLM (Gemma 3).
2.  **Consumer:** Writes the Python code for the task.
3.  **Reviewer:** Performs a "Senior Dev" critique of the code.
4.  **Supervisor:**  Calculates system health and lag for the Governor and Dashboard.
5.  **Persistence:** Saves the full task/solution/review trio to ./data/ai_history.csv.
5.  **Dashboard:** Streams the entire conversation and metrics live to the web.
 

## ðŸ› ï¸ Components & Topics
- **Topic `ai_topic`**: Tasks from Producer -> Consumer.
- **Topic `ai_solutions`**: Code from Consumer -> Reviewer.
- **Topic `ai_reviews`**: Critiques from Reviewer -> Dashboard.

## ðŸ–¥ï¸ The Dashboard (Visualizing the Mesh)
The system features a **Three-Column Real-Time Interface** designed for observability:

- **Column 1 (Producer):** Shows the raw task prompt generated by the brainstorming agent.
- **Column 2 (Consumer):** Displays the Python code produced by the developer agent.
- **Column 3 (Reviewer):** Displays the technical critique and bug-checks from the senior agent.


### Key Metrics
- **System Lag:** Located in the top right, this shows the pressure on the Consumer. A lag > 5 suggests you should scale your `consumer-agent` containers.

## ðŸš€ Deployment
### 1. Prerequisites
Docker & Docker Compose installed.

Ollama running locally with gemma3:1b installed.

Ensure Ollama is listening on http://host.docker.internal:11434.

### 2. Launch
```bash
docker compose up --build
### # Start the entire mesh in the background
docker compose up --build -d

# Scale the consumers to handle high volume
docker compose up -d --scale consumer-agent=3

# View live logs for all agents
docker compose logs -f

## ðŸ§ª System Stress Testing
To verify the **Adaptive Throttling** (Governor) logic:
1. Run the provided `stress_test.py` script.
2. This will inject 50 messages into the `ai_topic` instantly.
3. Observe the Dashboard:
    - **Lag Meter** climbs above 10.
    - **Status Light** changes to **YELLOW/THROTTLED**.
    - **Producer Agent** logs will show: `ðŸ›‘ LAG TOO HIGH. Throttling...`
4. Once the Consumers clear the backlog, the system will automatically return to **GREEN/ACTIVE**.

 